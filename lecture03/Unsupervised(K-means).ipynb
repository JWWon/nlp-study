{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1a22e8e8d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load('300features_40minwords_10text')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11986, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    " 1. 숫자로 단어를 표현\n",
    " 2. Word2Vec 모델은 어휘의 각 단어에 대한 feature 벡터로 구성, 'vectors'이라는 numpy list로 저장\n",
    " 3. vectors의 행 수는 모델 어휘의 단어 수\n",
    " 4. vectors의 열 수는 한 feature 벡터의 크기\n",
    " \"\"\"\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 단어 벡터 접근\n",
    "model.wv['flower'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiwoonwon/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K-means clustering:  226.3507058620453 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 단어 벡터에서 k-means를 실행하고 일부 클러스터를 찍어봄\n",
    "start = time.time()\n",
    "\n",
    "# 클러스터의 크기 \"k\"를 어휘 크기의 1/5나 평균 5단어로 설정\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = word_vectors.shape[0] // 5\n",
    "\n",
    "# K-means를 정의하고 학습\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# 프로세싱 시간을 구함\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(\"Time taken for K-means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['steven', 'spielberg', 'segal', 'donner', 'mcg']\n",
      "\n",
      "Cluster 1\n",
      "['forg']\n",
      "\n",
      "Cluster 2\n",
      "['shaft', 'clutch', 'skeleton', 'pinch', 'portal', 'hid', 'lemon', 'skyscrap', 'suitcas', 'dumpster', 'crate']\n",
      "\n",
      "Cluster 3\n",
      "['dick', 'lou', 'hawk', 'gregori', 'bud', 'randi', 'evan', 'dana', 'spencer', 'ethan', 'shepherd', 'duncan', 'carson', 'wheeler', 'moreland', 'morrow', 'corki', 'hackett', 'chum']\n",
      "\n",
      "Cluster 4\n",
      "['event', 'incid', 'occurr']\n",
      "\n",
      "Cluster 5\n",
      "['ach', 'sooth', 'etch']\n",
      "\n",
      "Cluster 6\n",
      "['north', 'africa', 'coast', 'pacif', 'northern', 'tokyo', 'patrol', 'atlant', 'iceland', 'northwest', 'alaska', 'carolina', 'pickup', 'gilligan', 'submerg', 'mediterranean', 'southwest']\n",
      "\n",
      "Cluster 7\n",
      "['detach', 'disconnect']\n",
      "\n",
      "Cluster 8\n",
      "['logo', 'catalog', 'billboard']\n",
      "\n",
      "Cluster 9\n",
      "['tale', 'fabl', 'propheci', 'fairytal', 'relic', 'backbon', 'parabl', 'lore', 'docudrama']\n"
     ]
    }
   ],
   "source": [
    "# 각 어휘 단어를 클러스터 번호에 매핑되도록 word/Index 사전을 생성\n",
    "idx = list(idx)\n",
    "names = model.wv.index2word\n",
    "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
    "\n",
    "# 첫 번째 클러스터의 처음 10개를 출력\n",
    "for cluster in range(10):\n",
    "    # 클러스터 번호 출력\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    \n",
    "    # 클러스터 번호와 클러스터에 있는 단어를 찍음\n",
    "    words = []\n",
    "    for i in range(len(list(word_centroid_map.values()))):\n",
    "        if list(word_centroid_map.values())[i] == cluster:\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    \n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/imdb/labeledTrainData.tsv', \n",
    "                    header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('../data/imdb/testData.tsv', \n",
    "                   header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append(\n",
    "        KaggleWord2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_centroids = np.zeros((train['review'].size, num_clusters), dtype='float32')\n",
    "train_centroids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    \"\"\"\n",
    "    centroid는 두 클러스터의 중심점을 정의한 다음 중심점의 거리를 측정한 것\n",
    "    \"\"\"\n",
    "    # 클러스터의 수는 word/centroid map 에서 가장 높은 클러스터 index와 같다\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype='float32')\n",
    "    \n",
    "    # 단어가 word_centroid_map에 있다면 클러스터의 수를 1개씩 증가\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data\n",
      "CPU times: user 35.7 s, sys: 212 ms, total: 35.9 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "    \n",
    "test_centroids = np.zeros((test['review'].size, num_clusters), dtype='float32')\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "    \n",
    "# 랜덤 포레스트를 사용하여 학습시키고 예측\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# train 데이터의 레이블을 통해 학습시키고 예측\n",
    "print(\"Fitting a random forest to labeled training data\")\n",
    "%time forest = forest.fit(train_centroids, train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 5s, sys: 3.14 s, total: 5min 8s\n",
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(forest, train_centroids, train['sentiment'],\\\n",
    "                                     cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 14.1 ms, total: 1.37 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%time result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163154880000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv로 저장\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\n",
    "output.to_csv(\"../data/imdb/submit_BagOfCentroids_{0:.5f}.csv\".format(score), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    12706\n",
       "1    12294\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentiment = output['sentiment'].value_counts()\n",
    "print(output_sentiment[0] - output_sentiment[1])\n",
    "output_sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
